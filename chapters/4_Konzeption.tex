\chapter{Konzeption}
In diesem Kapitel werde ich die grundlegende Konzeption der Arbeit vorstellen.

\section{Methodisches Vorgehen}
Es soll eine Beispielanwendung sowohl Containerisiert als auch Serverless entwickelt werden. Die Konzeption dieser beiden Anwendungen wird in den folgenden Abschnitten genauer beschrieben. 
Im Anschluss werden Performance Tests gegen beide Anwendungen durchgeführt und die Ergebnisse miteinander verglichen. Das methodische Vorgehen basiert dabei grundlegend auf den von Papadopoulus et. al. vorgestellten methodologischen Prinzipien für reproduzierbare Performanz-Evaluation im Cloud Computing Umfeld, die im folgenden beschrieben werden\cite{papadopoulos_methodological_2019}:

\begin{enumerate}
    \item[P1] Wiederholte Durchführung (Repeated experiments): Festlegen, wie viele Experimente mit der selben Konfiguration gemacht werden sollten. 
    \item[P2] Konfigurations-Abdeckung (Workload and configuration coverage): Experimente in verschiedenen Konfigurationen der relevanten Parameter durchführen. Beispielsweise verschiedene Hardware-Konfigurationen aber auch verschiedene Load-Testing-Typen (z.B. Spike Test)
    \item[P3] Experimenteller Aufbau (Experimental setup description): Beschreibung der Hardware- und Software (inklusive Version), und anderer Parameter die für das Experiment genutzt wurden und einen Einfluss auf dessen Ausgang haben können. Zusätzlich sollte die Beschreibung das genaue Ziel des Experiments beinhalten.
    \item[P4] Offener Zugang zu Artefakten (Open access artifact): Möglichst viele für die Experimente genutzten Konfigurationsdateien, Protokolle, etc. sollten versioniert und offen zugänglich sein.
    \item[P5] Probabilistische Ergebnisbeschreibung der gemessenen Performanz (Probabilistic result description of measured performance): Angemessenes Beschreiben und Visualisieren der Ergebnisse. Verwendung von Quantilen (z.B. Median oder 95. Quantil) und der Standardabweichung.
    \item[P6] Statistische Auswertung: Statistische Tests anwenden, um die Validität der Studie zu untermauern. Dieses Prinzip wird aus mangelnder Expertise und Zeit nicht genauer betrachtet, daher hier der Hinweis, dass die Ergebnisse dieser Arbeit eventuell nicht signifikant sind.
    \item[P7] Einheiten (Measurement units): Für alle Messungen die zugehörige Einheit angeben
    \item[P8] Kosten (Cost): Kostenmodell, Ressourcennutzung und abgerechnete Kosten angeben.
\end{enumerate}

\section{Konzeption der Testanwendung}
Als Testanwendung wird ein einfaches REST-Backend am Beispiel eines Notiz-Applikation entwickelt, wie es in der Praxis häufig verwendet wird. Der Service stellt folgende Routen bereit:  

\begin{enumerate}
    \item GET /notes: Das Auflisten aller verfügbarer Notizen
    \item GET /notes/\{id\}: Das Auflisten eines spezifischen Notiz mit der angegebenen id
    \item PUT /notes/\{id\}: Das Ändern einer spezifischen Notiz mit der angegebenen id
    \item POST /: Das Erstellen einer Notiz
\end{enumerate}

Um die Anwendung möglichst einfach zu gestalten und Unterschiede zwischen den beiden Technologien zu minimieren, wird zunächst auf die Verwendung einer Datenbank verzichtet. Stattdessen wird ein Delay von 50 Millisekunden für jeden Request eingebaut. 
Auch auf Authorisierungs-Mechanismen wird der Einfachheit halber verzichtet. Dies erlaubt es außerdem, von einer konstanten Benutzer-Rate auszugehen\cite{molyneaux_art_2014}. 
Als Runtime wird sowohl bei der Serverless- als auch bei der containerisierten Anwendung Node.js 12 verwendet.

\subsection{Serverless}
Für die Entwicklung der Serverless-Anwendung wird das, mit mehr als 38.000 Github-Stars populäre, Serverless-Framework\cite{noauthor_serverless_nodate} verwendet. Dabei handelt es sich um ein Open-Source Framework zur Entwicklung von Serverless Anwendungen. Es übernimmt die Erstellung von Anwendungs-Stacks, Rechteverteilung und Konfiguration bei allen gängigen Public-Cloud Anbietern wie AWS, Azure oder GCP. Da in dieser Arbeit AWS verwendet wird, erstellt Serverless automatisch einen Anwendungs-Stack mit AWS CloudFormation. Dabei wird ein AWS API-Gateway erstellt, das über die Routen der REST-Schnittstelle verfügt. Trifft eine Anfrage an eine der Routen ein, übernimmt das API-Gateway die Auslösung des passenden Events, wodurch die korrekte Lambda-Funktion mit den vom Benutzer im HTTP-Request übergebenen Parametern aufgerufen wird. 

Durch die Nutzung des Frameworks wird die Erstellung der Anwendung vereinfacht. Denn auch wenn man sich nicht mehr um Server kümmern muss, ist dennoch die Konfiguration des API-Gateways notwendig.

Abbildung \ref{fig:notes-serverless} zeigt das Setup der mit der Serverless-Architektur konzipierten Webanwendung.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{img/platzhalter.png}
    \caption[Serverless Notiz-Anwendung]{Serverless Notiz-Anwendung}
    \label{fig:notes-serverless}
\end{figure}

\subsection{Container}
Für die Entwicklung der containerisierten Anwendung wird das Express Framework verwendet, das mit über 51.000 Github-Stars eines der meist verwendeten Web-Frameworks für Node.js ist. Die Anwendung wird unter Nutzung der Docker-Engine mittels einer Dockerfile in ein Container-Image gebaut. Dieses Image wird in der AWS Elastic Container Registry (ECR) gespeichert, um es von dort aus weiter zu verwenden. 
Die Ausführung der Anwendung erfolgt auf der AWS Elastic Container Service (ECS) Plattform, einem von AWS bereitgestelltem Orchestrations-Tool. Es wird AWS Fargate benutzt, um die Erstellung eines Server-Clusters weg zu abstrahieren. Des Weiteren wird ein Elastic Load Balancer (ELB) vom Typ Application verwendet, um die eingehenden Requests auf die verschiedenen Container zu verteilen. 

Abbildung \ref{fig:notes-container} zeigt das Setup der mit der Container-Architektur konzipierten Notiz-Anwendung.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{img/platzhalter.png}
    \caption[Containerisierte Notiz-Anwendung]{Containerisierte Notiz-Anwendung}
    \label{fig:notes-container}
\end{figure}

\section{Konzeption der Tests}

\subsection{Testing-Tool}
Um die Performance der eben vorgestellten Anwendungen zu testen, wird das Performance-Testing Tool k6 \cite{noauthor_load_nodate} verwendet. Durch eine optimierte CPU Auslastung ermöglicht es, etliche virtuelle Benutzer zum Testen einer Anwendung erstellen lassen ohne die Notwendigkeit einer verteilen Ausführung\cite{noauthor_running_nodate}. Die Benutzer arbeiten parallel die in Sektion \ref{sec:use-cases} vorgestellten Use-Cases ab, bei denen verschiedene Requests an die REST-APIs der Services geschickt werden. 

Als Load-Generator wird eine in der AWS Region Paris (eu-west-3) stationierte EC2 Instanz des Typs t3.large verwendet.
Nach eigenen Angaben benötigt k6 für jeden virtuellen Benutzer und für einen kleinen Test eine Speicherauslastung zwischen 1 und 5 Megabyte\cite{noauthor_running_nodate}. Da die verwendete t3.large Instanz 8 Gigabyte RAM beinhaltet, sollten theoretisch mehr als 1600 und maximal 8000 gleichzeitige virtuelle Benutzer von der Testumgebung unterstützt werden. Die Instanz weist eine Netzwerkleistung von bis zu 5 Gbit/s auf, welche bei jeder Testausführung mittels des Tools iftop überwacht wird, um sicherzugehen, dass das Netzwerk nicht zum Flaschenhals wird. Auch die CPU Auslastung wird mittels htop überprüft.

\subsection{Metriken}
Das Testing-Tool k6 speichert für jeden Request an einen bestimmten API-Endpunkt unter anderem Werte folgender Metriken:
\begin{enumerate}
    \item VUS (virtual users): Die Anzahl der virtuellen Benutzer zum aktuellen Zeitpunkt 
    
    \item Response Time: Die Zeit vom Abschicken eines Requests bis zum Erhalt der Antwort in Millisekunden (ms). Dabei wird Aufwand für eventuelle DNS Lookups nicht mit einberechnet. 
    
    \item HTTP-Statuscodes: Geben den Status eines HTTP-Requests an. Wichtig für die Betrachtung sind vor allem die Codes:
        \begin{enumerate}
            \item 200 (OK): Der Request war erfolgreich.
            \item 503 (Service Unavailable): Der Server kann den Request nicht bearbeiten. Kann auftreten, wenn zu viele Requests bei einem Service eintreffen und dieser aufgrund mangelnder verfügbarer Ressourcen (CPU / Speicher) nicht in der Lage ist, die Anfrage zu bearbeiten.
        \end{enumerate}
        
    \item Request Rate: Die aggregierte Anzahl aller bereits abgeschlossenen Requests pro Sekunde (req / s)
\end{enumerate}

Da k6 keine eingebaute Visualisierungslösung bietet, werden diese Werte mittels der k6 Integration an den Cloud-Monitoring Service Datadog gesendet. Dort treffen sie mit von AWS Cloudwatch gesammelten Metriken zusammen, wodurch sie gemeinsam ausgewertet werden können.

Abbildung \ref{fig:test-architecture} zeigt das vollständige Setup der mit der Testing-Architektur.

Für zusätzliche Analysen und Vergleiche mehrerer Test-Ausführungen untereinander, werden die Ergebnisse jedes k6-Tests auch als CSV-Text-Datei exportiert. Im Anschluss können diese Dateien mit dem Data-Science Framework Pandas\cite{noauthor_pandas_nodate} für die Programmiersprache Python analysiert und verglichen werden. Der Source-Code der Analysen und die Ergebnis-Artefakte der Tests sind im Code-Repository dieser Arbeit zu finden.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{img/platzhalter.png}
    \caption[Test Architektur der Notiz-Anwendung]{Test Architektur der Notiz-Anwendung}
    \label{fig:test-architecture}
\end{figure}

\subsection{Test-Typen}
Bei Performance Tests können verschiedene Testing-Strategien eingesetzt werden.
\begin{enumerate}
    \item Load Testing: Dient der Evaluierung der System-Performance in Bezug auf die Anzahl der gleichzeitigen Benutzer oder der Requests-Rate. Es wird der Normalbetrieb des Systems simuliert.
    
    \item Stress Testing: Dient dazu, die Grenzen des Systems in den Punkten Verfügbarkeit und  Stabilität auszutesten. Man geht über den normalen Betrieb hinaus und eventuell so hoch dass das System der Last nicht mehr standhalten kann. Bei einem Stress-Test wird schrittweise die Last auf das System erhöht. So kann man zum Beispiel herausfinden, ob das System großen Anstürmen, wie z.B einem Sale-Event bei einem Online-Shop, Stand halten kann.
    
    \item Spike Testing: Eine Variante des Stress-Tests bei dem in einem kurzen Zeitraum extreme Last für das System-under-Test generiert wird. Im Gegensatz zu einem normalen Stress-Test, wird die Last hierbei nicht stufenweise, sondern plötzlich erhöht. Dadurch wird getestet, wie das System reagiert, wenn nicht ausreichend Zeit zur Skalierung der Infrastruktur bekommt.
\end{enumerate}

Es gibt noch weitere Test-Typen wie Smoke- oder Soak-Tests, die allerdings keine Relevanz für die Zielfrage dieser Arbeit haben und daher nicht betrachtet werden.

\subsection{Getestete Use-Cases}\label{sec:use-cases}
Die folgenden Use-Cases werden bei der Analyse der Systems-under-test evaluiert. Die nach einem Request angegebenen Zeitintervalle entsprechen der sogenannten "`Think Time"', also der Zeit die ein Benutzer normalerweise benötigt um sich für seine nächste Handlung zu entscheiden. Es wird, wie von Molyneaux empfohlen, mit einer ±10 prozentigen Abweichung versucht, eine realistischere Szenariodurchführung zu erreichen\cite{molyneaux_art_2014}.
\begin{itemize}
    \item Use-Case A: Alle Notizen abrufen \\
        Alle Notizen einmal abrufen. \\
        1. GET /notes -> 2s     \\

    \item Use-Case B: Notiz bearbeiten \\
        Erst alle Notizen, dann eine einzelne Notiz abrufen und bearbeiten. \\
        1. GET /notes -> 2s     \\
        2. GET /notes/1 -> 5s   \\
        3. PUT /notes/1         \\
        
    \item Use-Case C: Notiz falsch erstellt \\
    Der Benutzer ruft alle Notizen ab. Er entscheidet sich eine Notiz erstellen. Er ruft die erstellte Notiz ab und bemerkt dass er einen Fehler gemacht hat. Er bearbeitet die Notiz und speichert die Notiz erneut ab. \\
        1. GET /notes   -> 10s  \\
        2. POST /notes  -> 3s   \\
        3. GET /notes/1 -> 5s   \\
        4. PUT /notes/1
\end{itemize}


\subsection{Test-Variablen}
\begin{itemize}
    \item Use-Case: A / B / C
    \item Test Typ: Pipe-clean, Load, Stress, Spike-Test
    \item CPU und Memory des Systems: 128-...
    \item Skalierungs-Größe: 2 / 3 / 5 / 10 Task-Instanzen
    \item Anzahl an parallelen Benutzern: 25 / 50 / 75 / 100 ... 200
    \item Größe der Lambda Funktion
    \item Unterschiedliche Szenarien: Bei Lambda großer Unterschied 

    \item (Datenbank -> vielleicht Einfluss auf Memory?)
    \item (Skalierungs-Schwellen: Ab 60 / 70 / 75 Prozent CPU Auslastung)
    \item (Minimiertes JavaScript)
\end{itemize}


Es ergeben sich dadurch folgende Test-Szenarien:

CPU 128 / 256 / 512 für Use-Case A / B / C mit Instanzen 1 / 2 / 3 

\subsection{Test-Ablauf}
Nach Molyneaux \cite{molyneaux_art_2014} sollte zunächst ein Pipe-clean Test durchgeführt werden. Dabei wird jedes System von nur einem einzigen Benutzer für ein bestimmtes Szenario angefragt. Dadurch lassen sich Basiswerte für die betrachteten Metriken ableiten, welche in späteren Tests zu Vergleichen hinzugezogen werden können.

Anschließend wird ein Stress-Test für die Konfiguration der Container-Anwendung durchgeführt. Dies ist notwendig, da AWS Lambda ein von Natur automatisch skalierendes System ist, während ein Container alleine nicht skalieren kann. Um eine zu hohe Auslastung des Containers während des Tests zu vermeiden, wird der Stress-Test durchgeführt, um das VU-Limit für die aktuelle Container-Konfiguration zu erkennen. 
Im Anschluss wird mit diesem ermittelten VU-Limit ein Last-Test gegen beide System durchgeführt und die Ergebnisse beider SUTs miteinander verglichen.
