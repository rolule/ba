\chapter{Konzeption}
In diesem Kapitel werde ich die grundlegende Konzeption der Arbeit vorstellen.

\section{Methodisches Vorgehen}
Es soll eine Beispielanwendung sowohl Containerisiert als auch Serverless entwickelt werden. Im Anschluss werden Performance Tests gegen beide Anwendungen durchgeführt und die Ergebnisse miteinander verglichen.

\section{AWS Lambda}
Die Serverless Anwendung soll auf AWS Lambda ausgeführt werden. Dabei handelt es sich um ein Function-as-a-service Angebot von Amazon AWS.

\subsection{Funktionsweise}
Die Funktionsweise von Lambda lässt sich wie folgt beschreiben\cite{amazon_aws_aws_2020}:

\begin{enumerate}
\item Es tritt ein Event ein, dass die Ausführung der Lambda-Funktion anfordert. \\
    Dabei kann es sich um alle mögliche Ereignisse handeln. Zum Beispiel ein Dateiupload in S3 (AWS Object Store), einen CRON-Job oder im Falle eines Web Backends eine API-Aufruf mittels AWS API-Gateway.

\item Init Phase: Lambda erstellt automatisch eine Ausführungsumgebung (environment). \\
    Dabei handelt es sich um eine isolierte Umgebung, die für die Ausführung der Lambda- Funktion benötigt wird. Für diese Umgebung lassen sich einige Parameter von Anwender konfigurieren:
    
    \begin{enumerate}
        \item Die Runtime: Dabei handelt es sich um die verwendete Programmiersprache bzw. Framework. Als Optionen bietet Lambda beispielsweise Node.js 12.x, Java, Python oder Go.
        \item Die Arbeitsspeichergröße: Wie viel Speicher der ausgeführten Funktion bereitsteht. Die Mindestgröße beträgt hier 128MB; maximal sind etwas mehr als 10GB möglich. Proportional zur Speichergröße bestimmt Lambda ebenfalls die CPU Leistung, mit der die Funktion ausgeführt wird.
        \item Timeout: Die maximale Ausführungszeit der Funktion bevor Lambda sie automatisch beendet. Hier sind maximal 15 Minuten möglich.
    \end{enumerate}
    
    Die Umgebung wird mit den konfigurierten Ressourcen (Speicher, CPU) erstellt, der Code der Funktion geladen und entpackt und der Initialisierungscode der Lambda Funktion (nicht die Funktion an sich) ausgeführt. Dort lässt sich bspw. eine Datenbankverbindung aufbauen.
    
\item Invoke Phase: In dieser Phase wird die eigentliche Lambda Funktion, welche auch als Handler bezeichnet wird, ausgeführt. Eventuelle Rückgabewerte werden an den Aufrufer zurückgeliefert. Nachdem der Handler ausgeführt wurde, bleibt er verfügbar für weitere Anfragen. Der Initialisierungscode wird allerdings nicht mehr ausgeführt.
    
\item Shutdown Phase: Nachdem die Funktion für einige Zeit nicht mehr angefragt wurde, wird die Ausführungsumgebung wieder freigegeben. 
\end{enumerate}

\subsection{Cold- und Warmstart}
Nachdem der Handler in der Invoke-Phase ausgeführt wurde, wird die Ausführungsumgebung nicht sofort wieder freigegeben, sondern für einige Zeit vorgehalten, um weitere Anfragen schneller zu bearbeiten. Dies wird auch als Warmstart bezeichnet, da die Runtime-Umgebung bereits initialisiert ist und der Handler sofort aktiviert werden kann.
Muss die Umgebung nach einer eingetroffenen Anfrage erst noch initialisiert werden, also erst die Init-Phase ausgeführt werden, wird dies als Coldstart bezeichnet. Aufgrund des Mehraufwandes, erweist sich ein Coldstart als deutlich langsamer als ein Warmstart.

\subsection{Nebenläufigkeit}
Befindet sich der Handler einer Lambda-Funktion gerade in der Ausführung und es trifft ein weiteres Ereignis ein, startet Lambda zusätzlich zu der bereits laufenden Funktion eine weitere Ausführungsumgebung, um diese neue Anfrage zu verarbeiten. Da nun mehrere Umgebungen gleichzeitig ausgeführt werden, wird dies auch als Nebenläufigkeit (engl. concurrency) bezeichnet. Bei vielen gleichzeitigen Anfragen werden also so viele Umgebungen wie nötig erstellt. Der AWS Lambda Service erlaubt standardmäßig 1000 nebenläufige Funktionen pro AWS Region, dieser Wert lässt sich allerdings nach Absprache mit AWS erhöhen.

Durch die Nebenläufigkeit wird die Stärke von Lambda Funktionen bei der Skalierung deutlich. Gibt es einen Anfragesturm, werden automatisch neue Instanzen aufgesetzt, die diese Bearbeiten können. Bereits initialisierte Umgebungen werden wiederverwendet. Lässt die Anfrage nach, werden Umgebungen automatisch wieder heruntergefahren. 

Allerdings muss für jede neue Umgebung ist ein Coldstart durchgeführt werden der viel Zeit kostet und sich dem Benutzer als deutliche Latenz bemerkbar macht. Und auch für die Ausführungskosten stellen Coldstarts ein Problem dar, denn Lambda-Funktionen werden nach der Ausführungszeit in Millisekunden abgerechnet. Darum gilt es, Coldstarts möglichst zu vermeiden. Beispielsweise lassen sich mit Provisioned Concurrency bereits Funktionen vor-starten, die dann bei Bedarf nicht mehr einen Kaltstart erfordern. Kombiniert mit Autoscaling, das die provisionierte Kapazität an steigende Anfragezahlen anpasst, lässt sich so die Anzahl an Kaltstarts vermindern.

\subsection{Kosten}
Wie bereits erwähnt, wird die Ausführung von Lambda-Funktionen nach der Ausführungszeit in Millisekunden abgerechnet. Dabei zählt die Zeit eines Cold-Starts mit in die Berechnung ein. Bei der Berechnung zählt aber auch die konfigurierte Arbeitsspeichergröße mit in das Ergebnis ein. Derzeit berechnet AWS in der Region Frankfurt (eu-central-1) Kosten von 0,0000166667 US-Dollar pro GB-Sekunde\cite{noauthor_lambda_nodate}. Zusätzlich müssen pro 1.000.000 Ausführungen im Monat noch einmal 0,20 US-Dollar Anforderungsgebühren gezahlt werden.

\section{Konzeption der Testanwendung}
Als Testanwendung wird ein einfaches REST-Backend am Beispiel eines Notiz-Applikation entwickelt, wie es in der Praxis häufig verwendet wird. Der Service stellt folgende Routen bereit:  

\begin{enumerate}
    \item GET /notes: Das Auflisten aller verfügbarer Notizen
    \item GET /notes/\{id\}: Das Auflisten eines spezifischen Notiz mit der angegebenen id
    \item PUT /notes/\{id\}: Das Ändern einer spezifischen Notiz mit der angegebenen id
    \item POST /: Das Erstellen einer Notiz
\end{enumerate}

Um die Anwendung möglichst einfach zu gestalten und Unterschiede zwischen den beiden Technologien zu minimieren, wird zunächst auf die Verwendung einer Datenbank verzichtet. Stattdessen wird ein Delay von 50 Millisekunden für jeden Request eingebaut. 
Auch auf Authorisierungs-Mechanismen wird der Einfachheit halber verzichtet. Dies erlaubt es außerdem, von einer konstanten Benutzer-Rate auszugehen. 
Als Runtime wird in beiden Fällen Node.js 12 verwendet.

\subsection{Serverless}
Für die Entwicklung der Serverless-Anwendung wird das populäre (36.000 Github-Stars) Serverless-Framework\cite{noauthor_serverless_nodate} verwendet. Dabei handelt es sich um ein Open-Source Framework zur Entwicklung von Serverless Anwendungen. Es übernimmt die Erstellung von Anwendungs-Stacks, Rechteverteilung und Konfiguration bei allen gängigen Public-Cloud Anbietern wie AWS, Azure oder GCP. Da in dieser Arbeit AWS verwendet wird, erstellt Serverless automatisch einen Anwendungs-Stack mit AWS CloudFormation. Dabei wird ein AWS API-Gateway erstellt, das über die Routen der REST-Schnittstelle verfügt. Trifft eine Anfrage an eine der Routen ein, übernimmt das API-Gateway die Auslösung des passenden Events, wodurch die korrekte Lambda-Funktion mit den vom Benutzer übergebenen Parametern aufgerufen wird. 

Durch die Nutzung des Frameworks wird die Erstellung der Anwendung vereinfacht. Denn auch wenn man sich nicht mehr um Server kümmern muss, ist dennoch die Konfiguration des API-Gateways notwendig.

Abbildung \ref{fig:notes-serverless} zeigt das Setup der mit der Serverless-Architektur konzipierten Webanwendung.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{img/platzhalter.png}
    \caption[Serverless Notiz-Anwendung]{Serverless Notiz-Anwendung}
    \label{fig:notes-serverless}
\end{figure}

\subsection{Container}
Für die Entwicklung der containerisierten Anwendung wird das Express Framework verwendet, das mit über 51.000 Github-Stars eines der meist verwendeten Web-Frameworks für Node.js ist. Die Anwendung wird unter Nutzung der Docker-Engine mittels einer Dockerfile in ein Container-Image gebaut. Dieses Image wird in der AWS Elastic Container Registry (ECR) gespeichert, um es von dort aus weiter zu verwenden. 
Die Ausführung der Anwendung erfolgt auf der AWS Elastic Container Service (ECS) Plattform, einem von AWS bereitgestelltem Orchestrations-Tool. Es wird AWS Fargate benutzt, um die Erstellung eines Server-Clusters weg zu abstrahieren. Des Weiteren wird ein Elastic Load Balancer (ELB) vom Typ Application verwendet, um die eingehenden Requests auf die verschiedenen Container zu verteilen. 

Abbildung \ref{fig:notes-container} zeigt das Setup der mit der Container-Architektur konzipierten Notiz-Anwendung.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{img/platzhalter.png}
    \caption[Containerisierte Notiz-Anwendung]{Containerisierte Notiz-Anwendung}
    \label{fig:notes-container}
\end{figure}

\section{Konzeption der Tests}

\subsection{Testing-Tool}
Um die Performance der eben vorgestellten Anwendungen zu testen, wird das Performance-Testing Tool k6 \cite{noauthor_load_nodate} verwendet. Durch eine optimierte CPU Auslastung ermöglicht es, etliche virtuelle Benutzer zum Testen einer Anwendung erstellen lassen ohne dass eine verteile Ausführung benötigt wird\cite{noauthor_running_nodate}. Die Benutzer arbeiten dann parallel vordefinierte Test-Szenarien ab, bei denen verschiedene Requests an die REST-APIs der Services geschickt werden. 

Als Load-Generator wird eine in der AWS Region Paris (eu-west-3) stationierte EC2 Instanz des Typs t3.large verwendet.
Nach eigenen Angaben benötigt k6 für jeden virtuellen Benutzer und für einen kleinen Test eine Speicherauslastung zwischen 1 und 5 Megabyte\cite{noauthor_running_nodate}. Da die verwendete t3.large Instanz 8 Gigabyte RAM beinhaltet, sollten theoretisch mehr als 1600 und maximal 8000 gleichzeitige virtuelle Benutzer von der Testumgebung unterstützt werden. Die Instanz weist eine Netzwerkleistung von bis zu 5 Gbit/s auf, welche bei jeder Testausführung mittels des Tools iftop überwacht wird, um sicherzugehen, dass das Netzwerk nicht zum Flaschenhals wird. Auch die CPU Auslastung wird mittels htop überprüft.

\subsection{Metriken}
Das Testing-Tool speichert für jeden Request an einen bestimmten API-Endpunkt Werte folgender Metriken:
\begin{enumerate}
    \item VUS (virtual users): Die Anzahl der virtuellen Benutzer zum aktuellen Zeitpunkt 
    
    \item Response Time: Die Zeit vom Abschicken eines Requests bis zum Erhalt der Antwort in Millisekunden (ms). Dabei wird Aufwand für eventuelle DNS Lookups nicht mit einberechnet. 
    
    \item HTTP-Statuscodes: Geben den Status eines HTTP-Requests an. Wichtig für die Betrachtung sind vor allem die Codes:
        \begin{enumerate}
            \item 200 (OK): Der Request war erfolgreich.
            \item 503 (Service Unavailable): Der Server kann den Request nicht bearbeiten. Kann auftreten, wenn zu viele Requests bei einem Service eintreffen und dieser aufgrund mangelnder verfügbarer Ressourcen (CPU / Speicher) nicht in der Lage ist, die Anfrage zu bearbeiten.
        \end{enumerate}
        
    \item Request Rate: Die aggregierte Anzahl aller bereits abgeschlossenen Requests pro Sekunde (req / s)
\end{enumerate}

Da k6 keine eingebaute Visualisierungslösung bietet, werden diese Werte mittels der k6 Integration an den Cloud-Monitoring Service Datadog gesendet. Dort treffen sie mit von AWS Cloudwatch gesammelten Metriken zusammen, wodurch sie gemeinsam ausgewertet werden können.

Abbildung \ref{fig:test-architecture} zeigt das vollständige Setup der mit der Testing-Architektur.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{img/platzhalter.png}
    \caption[Test Architektur der Notiz-Anwendung]{Test Architektur der Notiz-Anwendung}
    \label{fig:test-architecture}
\end{figure}

\subsection{Test-Typen}
Darüber hinaus bietet k6 mehrere Test-Typen an. Diese bestimmen, wie viele Benutzer zu welchen Zeitpunkt Anfragen an den Service stellen. Von den fünf angebotenen Typen sind die folgenden relevant:
\begin{enumerate}
    \item Load Testing: Dient der Evaluierung der System-Performance in Bezug auf die Anzahl der gleichzeitigen Benutzer oder der Requests-Rate. Es wird der Normalbetrieb des Systems simuliert.
    
    \item Stress Testing: Dient dazu, die Grenzen des Systems in den Punkten Verfügbarkeit und  Stabilität auszutesten. Man geht über den normalen Betrieb hinaus und eventuell so hoch dass das System der Last nicht mehr standhalten kann. Bei einem Stress-Test wird schrittweise die Last auf das System erhöht. So kann man zum Beispiel herausfinden, ob das System großen Anstürmen, wie z.B einem Sale-Event bei einem Online-Shop, Stand halten kann.
    
    \item Spike Testing: Eine Variante des Stress-Tests bei dem in einem kurzen Zeitraum extreme Last für das System-under-Test generiert wird. Im Gegensatz zu einem normalen Stress-Test, wird die Last hierbei nicht stufenweise, sondern plötzlich erhöht. Dadurch wird getestet, wie das System reagiert, wenn nicht ausreichend Zeit zur Skalierung der Infrastruktur bekommt.
\end{enumerate}

Es gibt noch weitere Test Typen wie Smoke- oder Soak-Tests, die allerdings keine Relevanz für die Evaluierung der System-Performanz haben und daher nicht betrachtet werden.

\subsection{Test-Szenarien}
Die folgenden Test Szenarien werden für die Analyse durchgeführt. Die nach einem Request angegebenen Zeitintervalle entsprechen der sogenannten "`Think Time"', also der Zeit die ein Benutzer normalerweise benötigt um sich für seine nächste Handlung zu entscheiden. Es wird, wie von Molyneaux empfohlen, mit einer ±10 prozentigen Abweichung versucht, eine realistischere  Szenariodurchführung zu erreichen \cite{molyneaux_art_2014}.
\begin{itemize}
    \item Szenario A: Alle Notizen abrufen \\
        Alle Notizen einmal abrufen. \\
        1. GET /notes -> 2s     \\

    \item Szenario B: Notiz bearbeiten \\
        Erst alle Notizen, dann eine einzelne Notiz abrufen und bearbeiten. \\
        1. GET /notes -> 2s     \\
        2. GET /notes/1 -> 5s   \\
        3. PUT /notes/1         \\
        
    \item Szenario C: Notiz falsch erstellt \\
    Der Benutzer ruft alle Notizen ab. Er entscheidet sich eine Notiz erstellen. Er ruft die erstellte Notiz ab und bemerkt dass er einen Fehler gemacht hat. Er bearbeitet die Notiz und speichert die Notiz erneut ab. \\
        1. GET /notes   -> 10s  \\
        2. POST /notes  -> 3s   \\
        3. GET /notes/1 -> 5s   \\
        4. PUT /notes/1
\end{itemize}


\subsection{Test-Variablen}
\begin{itemize}
    \item Test Typ: Pipe-clean, Load, Stress, Spike-Test 
    \item Test Injection Type: gestufter Ramp-up, BigBang
    \item Test Dauer
    \item CPU und Memory des Systems: 128-...
    \item Skalierungs-Größe: 2 / 3 / 5 / 10 Task-Instanzen
    \item Anzahl an parallelen Benutzern: 25 / 50 / 75 / 100 ... 200
    \item Größe der Lambda Funktion
    \item Unterschiedliche Szenarien: Bei Lambda großer Unterschied 

    \item (Datenbank -> vielleicht Einfluss auf Memory?)
    \item (Skalierungs-Schwellen: Ab 60 / 70 / 75 Prozent CPU Auslastung)
    \item (Minimiertes JavaScript)
\end{itemize}

\subsection{Test-Reihenfolge}
\begin{enumerate}
    \item Pipe-clean Test: Nach Molyneaux \cite{molyneaux_art_2014} sollte zunächst ein Pipe-clean Test durchgeführt werden. Dabei wird jedes System von nur einem einzigen Benutzer für ein bestimmtes Szenario angefragt. Dadurch lassen sich Basiswerte für die betrachteten Metriken ableiten, welche in späteren Tests zu Vergleichen hinzugezogen werden können.
    \item 
\end{enumerate}
