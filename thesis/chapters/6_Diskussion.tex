\chapter{Diskussion der Ergebnisse}
In diesem Kapitel sollen die Ergebnisse aus der vorangegangenen Analyse diskutiert werden.
Allgemein ist in den Experimenten deutlich geworden, dass die Performance der Container-Anwendung die der Lambda-Funktionen übertrifft. Die Antwortzeit lag im Median bei fast allen Tests bei ca. 60ms. Nur wenn die CPU-Auslastung eines Container an die 100 Prozent erreichte, stiegen die Response-Times an. Dagegen lagen die Werte der Lambda-Funktionen bspw. bei den Pipe-Clean Tests um den Wert 100ms. Allerdings konnte die Performance von Lambda mit einer größerer Anzahl an Requests verbessert werden. Bei den Load- und Stress-Tests konnte so mediane Antwortzeiten von 76ms erreicht werden. Die kleinste Request-Dauer überhaupt lag bei Lambda, Fehler ausgenommen, bei 69,99ms. Größere Ausreißer gab es bei beiden Technologien. Meist wurden diese durch Fehler verursacht. Während bei Fargate ausschließlich HTTP 502 (Bad Gateway) auftritt, sind bei Lambda sowohl HTTP 500 (Internal Server Error) und HTTP 504 (Gateway Timeout) Fehler vorzufinden. Ähnlich zu den verlängerten Antwortzeiten sind die Fehler bei Fargate aber meistens nur bei hoher CPU-Auslastung ausgelöst worden. Zu einem kompletten Absturz des Servers oder HTTP 503 Statuscodes kam es in keinem Fall. 
Bei Lambda ist eine solche Auslastung durch die automatische Skalierung nicht möglich. Es ist daher sicher anzunehmen, dass die Fehler durch das API Gateway verursacht wurden und nichts mit den eigentlichen Funktionen zu tun haben. Generell traten aber bei beiden Services wenige Fehler auf. Die von Lambda sind allerdings mit einer Request-Dauer von bis zu 30 Sekunden als verheerender einzustufen.

Auch in der Varianz der Antwortzeit zeigen sich große Unterschiede der Technologien. Die Fargate Container zeigten schon in den Pipe-Clean Tests eine geringe Abweichung vom Mittelwert. Bei allen Konfigurationen und für alle Use-Cases betrug der Variationskoeffizient ca. 0,01. Bei den Lambda-Funktionen zeigte sich ein anderes Bild. Hier hatte sowohl die Funktionsgröße als auch der Use-Case einen Einfluss auf die Ergebnisse. In den Pipe-Clean Tests zeigte sich, dass Use-Case A einen Variationskoeffizient von ca. 0,23 bis 0,29 aufweist, während er bei Use-Case B zwischen 0,45 bis 0,52 schwankt. Während in den Pipe-Clean Tests keine Unterschiede zwischen den unterschiedlichen Funktions-Konfigurationen auffielen, sah es bei den Stress- und Load-Tests anders aus. Die 128MB Variante zeigte im Stress Test mit 600 VUs noch einen Abweichungskoeffizient von 0,22 (Use-Case A), während der Wert bei 256MB für den gleichen Test auf 0,17 schrumpfte. Dies scheint im Zusammenhang mit der Funktions-Dauer zu stehen, deren maximaler Wert bei der kleineren Konfiguration fast bei 600ms lag; bei der größeren aber nur noch knapp 240ms betrug. Zwischen 256MB und 512MB Variante gab es dann aber keine Unterschiede mehr in der Abweichung vom Mittelwert.

\section{Beantwortung der Forschungsfragen (RQ1 - RQ5)}
Im folgenden sollen die in Kapitel 4 vorgestellten Forschungsfragen anhand in der Analyse festgestellter Ergebnisse beantwortet werden.

Um die Anzahl der nebenläufigen Lambda-Funktionen zu ermitteln, die dem maximalen Load eines Containers entsprechen (RQ1), wurden mehrere Stress-Tests durchgeführt. Für die 128MB und 256MB Container-Instanzen konnte für Use-Case A ein maximaler Load von ca. 700 virtuellen Benutzern ermittelt werden. In den anschließenden Load-Tests mit bis zu 600 Benutzern wurde eine Nebenläufigkeit von maximal 69 Funktionen festgestellt. Bei der 512MB Konfiguration wurde eine Grenze von etwa 1.400 parallelen Benutzern ermittelt. Bei den Load-Tests mit 1200 Benutzern konnten bis zu 137 Lambda-Funktionen registriert werden. 
Für die anderen Use-Cases lag die Request-Rate deutlich unter der von Use-Case A. Das führte darum ebenso zu einer geringeren Nebenläufigkeit mit 60 bzw. 103 bei Use-Case B. 
Forschungsfrage RQ1 lässt sich also nicht eindeutig beantworten, da sich kein erkennbares Muster abzeichnet und die Zahlen zwischen gleichen Experimenten deutlich abweichen können. Darüber hinaus ist die Nebenläufigkeit von Lambda-Funktionen abhängig von der Laufzeit der Funktion, welche bei diesen Experimenten auf mindestens 50ms beschränkt war und darum in einer anderen Anwendung auch stark variieren könnte. Es lässt sich aber für RQ1 sagen, dass eine einzige Container-Instanz durchaus mehreren Hunderten, evtl. sogar Tausenden Lambda-Funktionen entsprechen kann.

Für die Forschungsfrage RQ2 wurden die beiden Anwendungen Load-Tests mit unterschiedlich schnellem Anstieg der Benutzerzahlen unterzogen. Es wurde vermutet, dass die Performance unter schnellem Anstieg bei beiden Systemen evtl. schlechter als die bei langsamen Anstieg wäre. Für die Performance des Containers konnte in den Experimenten keine Veränderung festgestellt werden. Bei den Lambda-Funktionen zeigte sich ein differenziertes Bild. Die Spike-Tests der 128MB Variante lösten einen verzögerten Anstieg der Antwortzeiten aus, welche bei Use-Case A noch ähnlich denen des Load-Tests, bei Use-Case B jedoch deutlich über denen des Load-Tests lagen. Bei den 256MB und 512MB Funktionen konnte jedoch überhaupt kein Unterschied zwischen schnellem und langsamen Unterschied festgestellt werden. 

Die Forschungsfrage RQ3 befasste sich mit der Nutzung größerer RAM und CPU-Werten für sowohl die Container-Anwendung als auch die Lambda-Funktionen. Vermutet wurde, dass bei einer Verdopplung der Container-CPU auch doppelt so viele Anfragen verarbeitet werden können. Dies konnte teilweise bestätigt werden. Bei einem Wechsel von 128MB auf 256MB konnte keine Veränderung der maximalen Benutzerzahlen festgestellt werden. Beide Container konnten für Use-Case A etwa 700 Benutzer bedienen, bevor die Antwortzeiten anstiegen. Vermutlich hängt dies damit zusammen, dass beide die gleichen Task-Gruppe von 512MB RAM und 0,25 vCPU zugewiesen bekamen. Dies war jedoch nötig, da AWS Fargate keine geringere Task-Gruppierung mit bspw. 128MB RAM und 0,125 vCPU zur Verfügung stellt. Um dieses Problem auszugleichen wurden beiden Containern harte Limits für CPU und RAM gesetzt. Es wird vermutet, dass die Limits ignoriert wurden und deshalb beide Zugang zu 0,25 vCPU hatten. Im Gegensatz dazu konnte beim Wechsel von 256MB auf 512MB die Hypothese (H2) bestätigt werden. Während der 256MB Container bei Use-Case A noch ein Limit von ca. 700 VUs hatte, konnten beim 512MB Container (mit 0,5 vCPU) in etwa 1400 VUs, also in etwa doppelt so viele Benutzer vor Steigen der Antwortzeit bedient werden. Daher kann die Hypothese teilweise bestätigt werden.

Für die Forschungsfrage RQ4 wurde die Performance von mehreren Container-Instanzen zu der von einzelnen Containern in Beziehung gestellt. Die Hypothese H3 war, dass die Anzahl der möglichen Requests mit jeder hinzugefügten Instanz linear ansteigt. Dies konnte mit Stress-Tests von zwei 256MB Instanzen und einer 512MB Instanz gezeigt werden. Die Hypothese kann also bestätigt werden.

Die letzte Hypothese war, dass sich ein Use-Case mit mehreren Endpunkten aufgrund der vermehrten Coldstarts negativ auf die Antwortzeiten der Lambda-Anwendung auswirkt (H4). Wie erwartet konnte bei der Container-Anwendung keine Veränderung festgestellt werden. Bei der Lambda-Funktion trat in den Pipe-Clean Tests kein Unterschied zwischen den Use-Cases auf. Auch bei Load-Tests der Lambda-Funktionen hatte die Anzahl der Endpunkte keinen Einfluss auf die Antwortzeit. Grund dafür scheint zu sein, dass die geringere Request-Rate bei den Use-Cases mit mehreren Endpunkten weniger Cold-Starts verursacht als bei der Funktion mit einem Endpunkt. In dieser Arbeit konnte die Vermutung also nicht bestätigt werden; bei der Nutzung anderer Use-Cases könnte aber ein anderes Ergebnis entstehen.

\section{Kosten}
In Sektion \ref{sec:kosten} wurden die Kostenmodelle der beiden Services verglichen. Es wurden Formeln zur Abschätzung der Nutzungskosten eines Services für einen Monat vorgestellt. Der große Unterschied zwischen beiden Technologien liegt in der Abrechnung der tatsächlich verwendeten Rechenzeit. Während bei Lambda nur die Laufzeit einer Funktion im Millisekunden-Bereich abgerechnet wird, zahlt man pro Container einen Preis für seine gesamte Laufzeit - auch wenn er keine Anfragen bearbeitet. Im Falle eines Containers bezahlt man also gleich viel wenn er mehr oder weniger ausgelastet ist. Vor allem zu Zeiten in denen die Auslastung gering ist, bspw. Nachts, ist die Nutzung eines Containers ein großer Nachteil. Der Vorteil von Lambda liegt darin, dass bei wenig Nutzung auch nur wenig zu zahlen ist. Bei einer hohen Auslastung kann ein Container allerdings erheblich günstiger sein als eine Lambda-Anwendung, denn die Kosten pro einer Millionen Aufrufe des API-Gateways fallen schwer ins Gewicht. Bei welcher Technologie man weniger bezahlt hängt also von der Auslastung der Anwendung ab. Wie in Sektion \ref{subsec:kosten-lambda} gezeigt wurde, lässt sich ungefähr bestimmen, wie viele monatliche Requests mit Lambda möglich sind, bevor es teurer wird als ein einzelner Container. Andererseits hat ein Container eine Grenze, wie viele Requests er pro Sekunde verarbeiten kann, bevor ein weiterer Container hinzugezogen werden (skaliert werden) muss. Für jeden weiteren Container fallen dann auch weitere Kosten an, allerdings nur so lange er benötigt wird. Dafür müssen aber auch unter Umständen komplizierte Skalierungs-Konfigurationen erstellt werden, was bei Lambda nicht unbedingt notwendig ist. Welche Technologie das bessere Preis-Leistungs-Verhältnis aufweist, lässt sich also nur im Einzelfall bestimmen.

Insgesamt fielen für die Tests dieser Arbeit laut dem AWS Cost Explorer ca. 80 Euro für die Nutzung des API Gateways an. Für Lambda waren es nur ca. 4 Euro. Die Gesamtkosten der Serverless-Anwendung berufen sich also auf ungefähr 85 Euro.
Im Gegensatz dazu wurden für die Container Anwendung nur ca. 15 Euro fällig, davon ca. fünf Euro für ECS mit Fargate und ca. zehn Euro für den Elastic Load Balancer.
Zusätzlich müssen noch Kosten in Höhe von ca. 25 Euro für die EC2-Instanzen, die als Load-Generator fungierten, hinzugerechnet werden.
Die Durchführung aller Tests in dieser Arbeit, veranschlagt also insgesamt Kosten von ca. 124 Euro.

\section{Implikationen für die Praxis}
Durch die in dieser Arbeit durchgeführten Experimente wurde deutlich, dass die Performance eines Container-Backends auf AWS Fargate zwar der einer Serverless-Anwendung auf AWS Lambda in Bezug auf die Antwortzeit überlegen ist. Allerdings liegt der Unterschied der beiden Technologien nur im Bereich von einigen Millisekunden. Wird eine zeitkritische Anwendung benötigt, also z.B. eine Banking- oder Trading-Plattform, sollte dennoch eher die Verwendung einer Container-Architektur erwägt werden.

Lambda bietet ein stabiles Skalierungsverhalten, das auch bei Spitzenlasten gut funktioniert. Auch Container lassen sich skalieren, jedoch ist ein nicht zu vernachlässigender Aufwand der konkreten Implementierung des Skalierungsverhaltens zu beachten. Wird ECS ohne Fargate oder ein Kubernetes-Service verwendet, muss sich zusätzlich um die Konfiguration des Clusters gekümmert werde. All das wird von Lambda vollständig übernommen, was zu enormen Einsparungen an Projektkosten führen könnte. 

Zusätzlich muss bei den Service-Kosten für den konkrete Anwendungsfall abgewogen werden, welche Technologie besser geeignet ist. Bei konstanter relativ hoher Last auf dem Service ist es wahrscheinlich, dass das API Gateway erhebliche Kosten verursacht und die Nutzung eines oder mehrerer Container sich wirtschaftlicher erweist. Bei nur kurzzeitig hoher Spitzenlast oder geringer Nutzung des Services ist in Bezug auf die Gebühren vermutlich die Nutzung der FaaS-Anwendung geeigneter, da sonst für nicht genutzte Kapazität des Containers bezahlt werden muss. 
Es ist also empfehlen, kontinuierlich die Last seiner Anwendung zu überwachen und mit dem Einsatz verschiedener Technologien wie Fargate oder Lambda zu experimentieren um Kosten in der Entwicklung und beim Betrieb der Anwendung zu sparen.

\section{Ausblick}
Es gibt viele verschiedene Variablen die Einfluss auf die Performance von Serverless- und Containerisierten-Anwendungen nehmen können. In dieser Arbeit wurde sich auf die Performance eines REST-Backends beschränkt. Dabei wurden verschiedene Container und Lambda-Größen untersucht und unterschiedliche Test-Szenarien durchgeführt. Es konnten aber unmöglich alle verschiedenen Konfigurationen betrachtet und evaluiert werden. Beispielsweise lassen sich die Größen der Lambda-Funktionen noch auf mehrere Gigabyte erweitern. Container lassen sich auf viele Wege deployen und betreiben, z.B. auf einem selbst gemanagten EC2 Cluster, mit ECS, einem EKS Cluster oder Elastic Beanstalk. Dazu gehört ebenfalls das unter Umständen komplexe Skalierungsverhalten einer Container-Anwendung (siehe Abschnitt \ref{subsec:skalierung-container}). Das in dieser Arbeit genutzte 50ms Timeout könnte variiert werden oder durch Nutzung echter Services wie z.B. DynamoDB ersetzt werden. Auch das Skalierungsverhalten von Lambda-Funktionen kann mit Autoscaling und provisionierter Nebenläufigkeit variiert werden (siehe \ref{subsec:skalierung-lambda}). 

Immer häufiger werden auch Mischformen von Container und Serverless. Beispielsweise bietet AWS seit Ende 2020 an, Container über AWS Lambda verfügbar zu machen\cite{noauthor_aws_nodate-1}. Darüber könnten Container und FaaS-Angebote verschiedener Cloud-Anbieter verglichen werden, zum Beispiel von Microsoft Azure oder Google Cloud Platform.

Die in dieser Arbeit durchgeführten Tests waren auf eine einzige Beispielanwendung beschränkt. Es lassen sich aber für jede beliebige Anwendung Performance-Tests durchführen. In Zukunft könnte das für diese Arbeit genutzte Testing und Analyse-System daher ausgebaut werden, damit es für jede beliebige Anwendung nutzbar ist. Dies kann Organisationen dabei helfen, die Nutzung ihrer Technologien besser zu evaluieren. Es könnten ebenfalls noch weitere Metriken aus AWS CloudWatch, wie z.B. die tatsächlichen Funktions-Kosten, in die Analyse integriert werden und somit den Vergleich beider Technologien noch weiter verbessern. Auch andere Services wie AWS EKS oder Elastic Beanstalk könnten mit diesem System getestet werden.

