\chapter{Diskussion der Ergebnisse}
In diesem Kapitel sollen die Ergebnisse aus der vorangegangenen Analyse diskutiert werden.
Im Allgemeinen ist in den Experimenten deutlich geworden, dass die Performance der Container-Anwendung die der Lambda-Funktionen übertrifft. Die Antwortzeiten lagen bei der Nutzung eines Containers im Median in fast allen Tests bei ca. 60ms. Nur wenn die CPU-Auslastung eines Container sich einhundert Prozent annäherte, stiegen die Response-Times an. Dagegen lagen die Werte der Lambda-Funktionen bspw. bei den Pipe-Clean Tests um einen Wert von 100ms. Allerdings konnte die Performance von Lambda mit einer größerer Anzahl an Requests verbessert werden. Bei den Load- und Stress-Tests konnten so mediane Antwortzeiten von in etwa 76ms erreicht werden. Die kleinste Request-Dauer die in allen Lambda-Tests gemessen wurde betrug, Fehler ausgenommen, 69,99ms. Größere Ausreißer vom Mittelwert gab es bei beiden Technologien. Meist wurden diese durch Fehler verursacht. Während bei Fargate ausschließlich HTTP 502 (Bad Gateway) auftrat, sind bei Lambda sowohl HTTP 500 (Internal Server Error) und HTTP 504 (Gateway Timeout) Fehler vorzufinden. Ähnlich zu den verlängerten Antwortzeiten, sind die Fehler bei Fargate aber meistens nur bei hoher CPU-Auslastung ausgelöst worden. Es gab aber auch einzelne Ausfälle bei niedriger Auslastung. Zu einem kompletten Absturz des Servers oder HTTP 503 Statuscodes kam es in keinem Fall. 
Bei Lambda ist eine solche Auslastung durch die automatische Skalierung nicht möglich. Es ist daher sicher anzunehmen, dass die Fehler durch das API-Gateway verursacht wurden und nichts mit den eigentlichen Funktionen zu tun haben. Generell traten aber bei beiden Services wenige Fehler auf. Die der Serverless-Anwendung sind allerdings mit einer Request-Dauer von bis zu 30 Sekunden als verheerender einzustufen.

Auch in der Varianz der Antwortzeit zeigten sich große Unterschiede der Technologien. Die Fargate-Container wiesen schon in den Pipe-Clean Tests eine äußerst geringe Abweichung vom Mittelwert auf. Bei allen Konfigurationen und für alle Use-Cases betrug der Variationskoeffizient dort nur etwa 0,01. Bei den Lambda-Funktionen hatte sowohl die Funktionsgröße als auch der Use-Case einen Einfluss auf die Ergebnisse. In den Pipe-Clean Tests wies Use-Case A einen Variationskoeffizient von ca. 0,23 bis 0,29 auf, während er bei Use-Case B zwischen 0,45 bis 0,52 schwankte. Bei Use-Case C waren es mit 0,59 bis 0,64 noch größere Abweichungen. Während in den Pipe-Clean Tests keine Unterschiede in der Varianz der unterschiedlichen Funktions-Konfigurationen auffielen, sah es bei den Stress- und Load-Tests anders aus. Die 128MB Variante zeigte im Stress Test mit 600 VUs noch einen Abweichungskoeffizient von 0,22 (Use-Case A), bei 256MB schrumpfte der Wert für den gleichen Test auf 0,17. Dies scheint im Zusammenhang mit der Funktions-Dauer zu stehen, deren maximaler Wert bei der kleineren Konfiguration fast bei 600ms lag, bei der größeren aber nur noch knapp 240ms betrug. Zwischen 256MB und 512MB Variante gab es allerdings keine Unterschiede mehr in der Abweichung vom Mittelwert; nur die maximale Funktionsdauer sank noch weiter. 

\section{Beantwortung der Forschungsfragen (RQ1 - RQ5)}
Im Folgenden sollen die in Kapitel \ref{ch:konzeption} vorgestellten Forschungsfragen anhand in der Analyse festgestellter Ergebnisse beantwortet werden.

Um die Anzahl der nebenläufigen Lambda-Funktionen zu ermitteln, die dem maximalen Load eines Containers entsprechen (\hyperref[tab:research-questions]{RQ1}), wurden mehrere Stress-Tests durchgeführt und dadurch das \ac{VU}-Limit der Container-Konfigurationen ermittelt. Für die 128MB und 256MB Container-Instanzen konnte für Use-Case A ein maximaler Load von ca. 700 virtuellen Benutzern ermittelt werden. In den anschließenden Load-Tests mit bis zu 600 Benutzern wurde eine Nebenläufigkeit von maximal 72 Funktionen festgestellt. Bei der 512MB Konfiguration wurde eine Grenze von etwa 1.400 parallelen Benutzern ermittelt. Bei den Load-Tests mit 1.200 Benutzern konnten bis zu 137 Lambda-Funktionen registriert werden. 
Für die anderen Use-Cases lag die Request-Rate deutlich unter der von Use-Case A. Dadurch konnten z.B. bei der 512MB Funktion für Use-Case B mehr als 2.200 Benutzer, bei Use-Case C sogar mehr als 2.800 Benutzer registriert werden, bevor die Antwortzeiten anstiegen. Die geringere Request-Rate führte auch zu einer geringeren Nebenläufigkeit bei den Load-Tests der Use-Cases B und C. Daher sollte bei zukünftigen Experimenten von einer ähnlichen Request-Rate anstatt einer gleichen Anzahl der Benutzer ausgegangen werden.
Darüber hinaus ist die Nebenläufigkeit von Lambda-Funktionen abhängig von der Laufzeit der Funktion, welche bei diesen Experimenten auf mindestens 50ms beschränkt war und darum in einer anderen Anwendung auch stark variieren könnte. In Zukunft könnten daher verschieden in ihrer Laufzeit beschränkte Lambda-Funktionen auf ihre Nebenläufigkeit untersucht werden.
Forschungsfrage \hyperref[tab:research-questions]{RQ1} lässt sich also nicht eindeutig beantworten, da sie sich als eindeutig anwendungsspezifisch erwiesen hat. Es lässt sich aber feststellen, dass eine einzige Container-Instanz durchaus mehreren Hunderten, evtl. sogar Tausenden Lambda-Funktionen entsprechen kann.

Für die Forschungsfrage \hyperref[tab:research-questions]{RQ2} wurden die beiden Anwendungen Load-Tests mit unterschiedlich schnellem Anstieg der Benutzerzahlen unterzogen. Für die Performance des Containers konnte in den Experimenten keine Veränderung bei dem steilen Anstieg festgestellt werden. Bei den Lambda-Funktionen zeigte sich ein differenziertes Bild. Die Spike-Tests der 128MB Variante lösten einen verzögerten Anstieg der Antwortzeiten aus, welche bei Use-Case A noch ähnlich zu denen des Load-Tests; bei Use-Case B jedoch deutlich über denen des Load-Tests lagen. Bei den 256MB Funktionen konnte nur noch für Use-Case C ein geringer Unterschied festgestellt werden und bei den 512MB Funktionen war gar keine Diskrepanz mehr zwischen schnellem und langsamen Anstieg erkennbar. Bei höheren Nutzerzahlen könnte dieser Effekt jedoch eventuell größer ausfallen. Es ist daher noch unklar, wann genau es zu Unterschieden zwischen schnellem und langsamen Anstiegen bei Lambda-Funktionen kommt.

Die Forschungsfrage \hyperref[tab:research-questions]{RQ3} befasste sich mit der Nutzung größerer RAM und CPU-Werten für sowohl die Container-Anwendung als auch die Lambda-Funktionen. Bei einem Wechsel von 128MB auf 256MB konnte bei der \linebreak Container-Anwendung keine Veränderung der maximalen Benutzerzahlen festgestellt werden. Beide Container konnten z.B. für Use-Case A etwa 700 Benutzer bedienen, bevor die Antwortzeiten anstiegen. Vermutlich hängt dies damit zusammen, dass beide die gleichen Task-Gruppe von 512MB RAM und 0,25 vCPU zugewiesen bekamen. Dies war nötig, da \ac{AWS} Fargate keine geringere Task-Gruppierung mit bspw. 128MB RAM und 0,125 vCPU zur Verfügung stellt. Um dieses Problem auszugleichen wurden beiden Containern harte Ressourcen-Limits für CPU und RAM gesetzt. Es wird vermutet, dass die Limits ignoriert wurden und deshalb beide Zugang zu 0,25 vCPU hatten. 
Im Gegensatz dazu, konnte beim Wechsel von 256MB auf 512MB eine Veränderung erkannt werden. Während der 256MB Container bei Use-Case A noch ein Limit von ca. 700 VUs hatte, konnten beim 512MB Container (mit 0,5 vCPU) in etwa 1.400 VUs, also in etwa doppelt so viele Benutzer vor Steigen der Antwortzeit bedient werden. Eine Verbesserung in den Antwortzeiten war allerdings nicht zu erkennen. 
Bei den Lambda-Funktionen zeigten sich einige Unterschiede in der Performance verschiedenen Konfigurationsgrößen. Bei den Pipe-Clean Tests lagen noch alle Metriken für alle Varianten bei ähnlichen Werten. Lediglich die maximale Funktionsdauer verbesserte sich eindeutig zwischen den Größen. Die Stress-Tests der 256MB Funktion zeigten dagegen eine deutliche Verbesserung der Antwortzeiten im Vergleich zu denen der 128MB Funktion. Auch die Abweichung vom Mittelwert wurde geringer. Außerdem wurde das Skalierungsverhalten bei den größeren Funktionen resilienter gegenüber schnellem Benutzeranstieg. Zwischen der 256MB und der 512MB Variante gab es aber bis auf die niedrigere maximale Funktionsdauer keine erkennbaren Verbesserungen mehr.

Für die Forschungsfrage \hyperref[tab:research-questions]{RQ4} wurde die Performance von mehreren \linebreak Container-Instanzen zu der von einzelnen Containern in Beziehung gestellt. Bei Vergleich der Stress-Tests von zwei 256MB Instanzen und einer 512MB Instanz konnte gezeigt werden, dass beide Konfigurationen ungefähr die gleichen Limits an Benutzerzahlen aufwiesen. In den anschließenden Load-Tests gab es keine erkennbaren Unterschiede in den Antwortzeiten. Auffällig war, dass die Konfiguration mit zwei Containern eine höhere CPU-Auslastung aufwies als der einzelne Container. Dies könnte sehr wohl auf einen Unterschied in den Benutzerzahlen hindeuten, da die Vollauslastung des Clusters eventuell früher erreicht werden könnte. In Zukunft könnte dieser Effekt also noch detaillierter erforscht werden.

Untersuchungsgegenstand der letzten Forschungsfrage \hyperref[tab:research-questions]{RQ5} war, die Auswirkungen der Anzahl von Endpunkten eines Anwendungsfalls auf die Performance der Anwendungen zu untersuchen. Bei der Container-Anwendung konnte in keinem Fall eine Veränderung der Antwortzeiten zwischen den Use-Cases festgestellt werden. Bei der Lambda-Funktion trat in den Pipe-Clean Tests kein Unterschied zwischen den Use-Cases auf, was darauf zurückzuführen ist, dass nur jeweils ein Coldstart pro Endpunkt benötigt wurde. 
Anders sah es bei den Stress- und Load-Tests aus. Dort hatte die Anzahl der Endpunkte durchaus einen Einfluss auf die Entwicklung der Antwortzeit. Bei den Stress-Tests der 128MB Funktionen sanken die Antwortzeiten bei mehreren Endpunkten langsamer auf das untere Niveau ab, als die eines Endpunktes. Bei den größeren Funktionen trat dieser Effekt nicht auf. Allerdings erreichen die Use-Cases mit mehreren Endpunkten in fast allen Load- und Stress-Tests eine allgemein höhere Antwortzeit. Es traten aber auch Schwankungen auf, bei denen bspw. der Verlauf von Use-Case B unter dem von Use-Case A lag. Tendenziell lässt aber der Vergleich der jeweils besten Verläufe vermuten, dass die Anzahl der Endpunkte einen Einfluss auf die Performance der Lambda-Anwendung hat. Allerdings konnten in den Tests dieser Arbeit nur geringfügige Differenzen in den Antwortzeiten festgestellt werden. 

\section{Skalierung und Optimierung}
Da Container auf jedem Computersystem mit Unterstützung einer Container-Runtime betrieben lauffähig sind, bieten sich vielfältige Wege an eine containerisierte Anwendung zu betreiben und zu skalieren. Deshalb kann in dieser Arbeit nicht auf alle eingegangen werden. Meist wird ein Container-Orchestrations-Tool verwendet, um das Verteilen der Container auf \linebreak Computer-Clustern zu automatisieren. Bei Kubernetes gibt es den \ac{HPA}, der die automatische Skalierung von Pods in Abhängigkeit der CPU-Auslastung ermöglicht\cite{noauthor_horizontal_nodate}. Auch bei Nutzung des \ac{AWS} \ac{EKS} ist ein automatische Skalierung mittels \ac{HPA} möglich\cite{noauthor_horizontal_nodate-1}. Des Weiteren ist es eine Möglichkeit, einen Vertical Pod Autoscaler (VPA) zu verwenden, um automatisch die Ressourcen einzelner Container zu skalieren\cite{noauthor_vertical_nodate}. Es kann auch ein Cluster-Autoscaler verwendet werden, um zusätzlich zu der Anzahl der Container auch die Anzahl der Kubernetes-Knoten zu skalieren\cite{noauthor_cluster_nodate}.
Auf die Performance-Tests all dieser Konfigurationen kann in dieser Arbeit unmöglich eingegangen werden, da es zum einen zu viele Möglichkeiten der Skalierung und zum anderen auch zu viele Wege zur Optimierung gibt. Beispielsweise lässt sich die Schwelle der CPU-Auslastung einstellen, ab der die Anzahl der Container hoch oder runter skaliert werden soll. Liegt die Schwelle zu niedrig, skaliert das System evtl. zu früh und die Kosten steigen. Liegt die Schwelle zu hoch, skaliert das System evtl. zu spät und die Performance sinkt.

Um die Einstellung eines Clusters nicht vornehmen zu müssen, wurde in dieser Arbeit \ac{ECS} mit dem Fargate-Starttyp verwendet. Fargate verwaltet die Knoten des Clusters automatisch. Der ordinäre Weg der Cluster-Erstellung mit \ac{ECS} wäre, mehrere Virtuelle Maschinen zu starten, zu konfigurieren und jede dem Cluster zuzuweisen. Durch die Nutzung von Fargate kann die Konfiguration von Container-Clustern also erheblich vereinfacht werden. \ac{ECS} und Fargate bieten mit Auto-Scaling eine weitere Möglichkeit der automatischen Skalierung an. Diese funktioniert zusammen mit \ac{AWS} CloudWatch Alarmen. Dazu lässt sich eine von zwei verschiedenen Policies einstellen.

Bei einer Step Scaling Policy lassen sich, ähnlich wie bei Kubernetes \ac{HPA}, Grenzen für die Skalierung auf Basis von CloudWatch Metriken, also bspw. CPU- oder Arbeitsspeicher-Auslastung, festlegen. CloudWatch überprüft\linebreak dann in bestimmten Intervallen die Metriken und schlägt Alarm, wenn eine Metrik über oder unter dem spezifizierten Schwellwert liegt. Fargate reagiert auf den Alarm und skaliert die Anzahl der Container innerhalb des Clusters hoch oder herunter\cite{noauthor_amazon_nodate}. 

Bei einer Target Tracking Policy, lässt sich für Metriken ein Ziel festlegen, das eingehalten werden soll. Beispielweise könnte man ein Ziel von 75\% CPU-Auslastung festlegen. Mithilfe der CloudWatch Alarme wird dann versucht, dieses Ziel möglichst einzuhalten\cite{noauthor_amazon_nodate}.

Problematisch ist allerdings die Größe des Intervalls, in dem die Metriken überprüft werden können. Standardmäßig kann dies nur alle fünf Minuten erfolgen. Träfe die Container-Anwendung eine plötzliche Spitzenlast, würde es fünf Minuten dauern, bis reagiert und die Kapazität erhöht werden könnte. Da es sich bei der CPU-Auslastung um eine Standard-Metrik handelt, lässt sich dieses Intervall auf bis zu einer Minute herab senken\cite{noauthor_cloudwatch_concepts}. Dies ist allerdings, verglichen mit den Skalierungsmöglichkeiten von \ac{AWS} Lambda, noch immer langsam für Anwendungen die schnell auf hohe Lasten reagieren müssen. Eine weitere Senkung des Intervalls auf bis zu zehn Sekunden ist nur mit den sogenannten hochauflösenden Custom-Metriken möglich. Dafür fallen aber wiederum die dreifachen Kosten der Nutzung einer Standardmetrik an\cite{noauthor_cloudwatch_preise}.

Wichtig für eine schnelle Skalierung kann auch die Größe des Containers-Abbilds sein. Da bei jeder neu hinzugefügten Container-Instanz das \linebreak Container-Image von der Registry heruntergeladen werden muss\cite{noauthor_amazon_nodate}, ist es notwendig, die Image-Größe möglichst zu minimieren. Zum Einsatz kommende Techniken sind hierbei beispielsweise die Nutzung von Alpine-Images als Basis-Image, einer besonders kleinen Linux-Distribution, Multi-Stage \linebreak Builds, mit denen unnötige Dateien entfernt werden können, oder Layer-Merging, bei dem die Anzahl der Image-Layer minimiert wird\cite{noauthor_how_nodate}.

Lambda ist dagegen ein von Natur aus horizontal skalierendes System. Daher müssen vom Entwickler theoretisch keine Einstellungen vorgenommen werden, um eine hochverfügbare Anwendung zu erschaffen. Um Cold-Starts möglichst zu vermeiden, lässt sich die gewollte Nebenläufigkeit allerdings auch schon vor einem Benutzer-Ansturm definieren. Diese Einstellungsmöglichkeit wird als Provisioned Concurrency bezeichnet. Lambda startet in diesem Fall bereits Funktionen vor, die dann bei Bedarf keinen Kaltstart mehr erfordern, sondern direkt einen Warmstart durchführen können. Kombinieren lässt sich dies mit Autoscaling, um die provisionierte Kapazität an steigende Anfragezahlen anzupassen\cite{amazon_aws_aws_2020}. 

Auch die Größe des Arbeitsspeicher einer Lambda-Funktion kann einen Einfluss auf das Skalierungsverhalten nehmen. Da eine Funktion mit größerem Speicher auch mehr CPU zugewiesen bekommt, können Anfragen, bei besonders CPU-intensiven Aufgaben, schneller verarbeitet werden und es werden eventuell weniger nebenläufige Funktionen und damit weniger Cold-Starts benötigt. Es gibt Tools wie "`AWS Lambda Power Tuning"'\cite{casalboni_alexcasalboniaws-lambda-power-tuning_2021}, die es für solche Funktionen ermöglichen, die beste Konfiguration zu finden.

Es gibt darüber hinaus vielfältige Wege, die Coldstart Zeit einer Lambda-Funktion zu verringern. Ähnlich zum einem Container-Image, hat auch die Größe einer Lambda-Funktion Einfluss auf die Startzeit, denn bei jedem Coldstart muss der Funktions-Code aus \ac{S3} in den neuen Lambda-Container geladen werden. Bei einigen Programmiersprachen wie Java oder .NET macht ebenfalls die Konfiguration des Lambda-Arbeitsspeichers einen großen Unterschied bei der Schnelligkeit eines Coldstarts\cite{malishev_aws_2019}. Wie in den Tests dieser Arbeit bestätigt wurde, ist dies bei Node.js allerdings nicht der Fall.

\section{Kosten}
In Sektion \ref{sec:kosten} wurden die Kostenmodelle der beiden genutzten Services verglichen. Es wurden Formeln zur Abschätzung der Nutzungskosten eines Services für einen Monat vorgestellt. Der große Unterschied zwischen beiden Technologien liegt in der Abrechnung der tatsächlich verwendeten Rechenzeit. Während bei Lambda nur die Laufzeit einer Funktion im Millisekunden-Bereich abgerechnet wird, zahlt man pro Container einen Preis für seine gesamte Laufzeit - auch wenn er keine Anfragen bearbeitet. Für einen Container bezahlt man also gleich viel egal ob er mehr oder weniger ausgelastet ist. Vor allem zu Zeiten in denen die Auslastung gering ist, bspw. in der Nacht, ist die Nutzung eines Containers ein großer Nachteil. Der Vorteil von Lambda liegt darin, dass bei geringer Nutzung auch nur wenige Kosten anfallen. 
Im Gegensatz dazu, kann bei einer hohen Last die Nutzung eines Container erheblich günstiger sein als eine Lambda-Anwendung, denn die Kosten pro einer Millionen Aufrufe des API-Gateways fallen schwer ins Gewicht. Bei welcher Technologie man weniger bezahlt, hängt also von der konkreten Auslastung des Services ab. Wie in Sektion \ref{subsec:kosten-lambda} gezeigt wurde, lässt sich ungefähr bestimmen, wie viele monatliche Requests mit Lambda und API-Gateway möglich sind, bevor die Kosten die eines einzelnen Containers überschreiten. Andererseits hat ein Container eine Grenze, wie viele Requests er pro Sekunde verarbeiten kann, bevor ein weiterer Container hinzugezogen (skaliert) werden muss. Für jeden weiteren Container fallen auch weitere Kosten an, allerdings nur so lange er benötigt wird. Dafür müssen aber unter Umständen komplexe Skalierungs-Konfigurationen erstellt werden, was bei Lambda nicht notwendig ist. Hinzu kommen ebenso die ungewissen Nutzungskosten eines \ac{ELB}. Welche Technologie das bessere Preis-Leistungs-Verhältnis aufweist, lässt sich also nur im Einzelfall bestimmen. Ist die Anzahl der Anfragen oder der Benutzer der Anwendung bekannt, kann es aber für Organisationen hilfreich sein, die Kosten mit den vorgestellten Formeln abzuschätzen und mit Performance-Tests zu evaluieren, um die passendere Alternative auszuwählen. 

Um Prinzip \hyperref[tab:principles]{P8} zu erfüllen, werden im folgenden die tatsächlich für diese Arbeit angefallenen Kosten angegeben, um für eine erneute Durchführung der Experimente eine Einschätzung der nötigen Ressourcen zu geben. 
Insgesamt fielen für die Tests dieser Arbeit laut des \ac{AWS} Cost-Explorers 215,40 US-Dollar für die Nutzung des API-Gateways an. Für Lambda waren es nur 19,91 Dollar. Die Gesamtkosten der Serverless-Anwendung berufen sich also auf ungefähr 235 Dollar.
Im Gegensatz dazu wurden für die Container-Anwendung nur ca. 33 Dollar fällig, davon 12,94 Dollar für \ac{ECS} mit Fargate und 20,26 Dollar für den \ac{ELB}.
Zusätzlich müssen noch Kosten in Höhe von 48,52 Dollar für die \ac{EC2}-Instanzen, die als Load-Generator fungierten, hinzugerechnet werden. Es muss darauf hingewiesen werden, dass diese Kosten von der Betriebsdauer der \ac{EC2} Instanzen und des \ac{ECS}-Clusters abhängig sind. Des Weiteren fallen noch Kosten von 45,65 Dollar für \ac{AWS} CloudWatch an.
Die Durchführung aller Tests in dieser Arbeit veranschlagte also insgesamt Kosten von ca. 317 Dollar. Dass das API-Gateway alleine fast 60 Prozent der Gesamtkosten veranschlagte und Fargate nur in etwa neun Prozent, zeigt, dass die Nutzung von Container eindeutig effizienter ist wenn beide Services die gleiche Last bearbeiten müssen und der Container der Last gewachsen ist.

\section{Implikationen für die Praxis}
Durch die in dieser Arbeit durchgeführten Experimente wurde deutlich, dass die Performance eines Container-Backends auf \ac{AWS} Fargate der einer Serverless-Anwendung mit \ac{AWS} Lambda und API-Gateway in Bezug auf die Antwortzeiten überlegen ist. Allerdings liegt der Unterschied der beiden Technologien nur im Bereich von einigen Millisekunden. Wird eine zeitkritische oder in ihren Antwortzeiten konsistente Anwendung benötigt, bspw. eine Banking- oder Trading-Plattform, sollte dennoch eher die Verwendung einer Container-Architektur bevorzugt werden.

Lambda bietet ein stabiles Skalierungsverhalten, bei dem nach anfänglichen von Coldstarts verursachten Spitzenwerten beinahe konsistente Antwortzeiten erreicht werden und das auch bei schnellem Last-Anstieg gut zu funktionieren scheint. Auch Container lassen sich skalieren, jedoch muss der nicht zu vernachlässigende Aufwand der konkreten Implementierung des Skalierungsverhaltens beachtet werden. Wird \ac{ECS} ohne Fargate oder ein Kubernetes-Service verwendet, muss sich zusätzlich um die Konfiguration des Clusters gekümmert werden. All das wird von Lambda vollständig übernommen, was zu enormen Einsparungen an Entwicklungskosten führen kann. Ist bereits eine Container-Architektur für den Betrieb einer anderen Anwendung aufgebaut worden oder die Nutzung begleitender containerisierter Services (z.B. einer bestimmten Datenbank) unabdingbar, kann eine Container-Anwendung vorzuziehen sein. Auch die Nutzung von Fargate erleichtert den Betrieb von Container-Clustern, führt aber zu Einschränkungen in Bezug auf die Konfiguration des Skalierungsverhaltens.

Bei den Service-Kosten muss abgewogen werden, welche Technologie für den konkreten Anwendungsfall und unter Betrachtung der normalen Last der Anwendung besser geeignet ist. Bei konstanter, relativ hoher Last auf dem Service ist es wahrscheinlich, dass das API-Gateway erhebliche Kosten verursacht und die Nutzung eines oder mehrerer Container sich als wirtschaftlicher erweist. An dieser Stelle könnte man die Verwendung von alternativen API-Gateways, bspw. Kong\cite{noauthor_kongkong_2021}, erwägen. Hierbei muss jedoch beachtet werden, dass zusätzliche Gebühren für das Hosting und die Integration dieser Alternativen anfallen. Bei nur kurzzeitig hoher Spitzenlast oder geringer Nutzung des Services ist in Bezug auf die Gebühren vermutlich die Nutzung der \ac{FaaS}-Anwendung geeigneter, da ansonsten für nicht genutzte Kapazität des Containers gezahlt werden muss. 
Besonders bei der erstmaligen Einführung einer Anwendung könnte es aufgrund der unbekannten Nutzungsrate günstiger sein, ein Serverless-Backend zu verwenden, um nicht für ungenutzte Kapazität zahlen zu müssen. Ist die normale Last der Applikation bekannt, können die Kosten der bestehenden Anwendung evaluiert werden und eventuell eine Container-Architektur aufgebaut werden. Andererseits wurde in den Kostenmodell-Vergleichen dieser Arbeit deutlich, dass einige Millionen-Aufrufe einer Lambda-Funktion monatlich möglich sind, bevor die Kosten einer einzigen Container-Instanz erreicht werden, was für viele kleinere Services ausreichend sein könnte.
Des Weiteren ist zu empfehlen, stets die Last seiner Anwendung zu überwachen und mit dem Einsatz verschiedener Technologien wie Fargate oder Lambda zu experimentieren, um Kosten in der Entwicklung und beim Betrieb der Anwendung einzusparen. Es wird vorgeschlagen im Falle eines Backends die Kernfunktionalität zu extrahieren, um sie in beiden Services nutzbar zu machen. Diese könnten dann in einer Art A/B-Testing Strategie parallel oder abwechselnd betrieben und kontinuierlich evaluiert werden.