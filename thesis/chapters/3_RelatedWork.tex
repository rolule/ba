\section{Verwandte Arbeiten}
Serverless ist allgemein ein noch junges Thema, das erst mit der Einführung von \ac{AWS} Lambda im Jahre 2014 Aufmerksamkeit erfuhr\cite{ken_owens_cncf_2018}. Es gibt bereits einige Studien zu der Performance von Serverless-Anwendungen, in den meisten Studien wurden jedoch nur Micro-Benchmarks betrachtet, d.h Tests, die nur einen Aspekt untersuchen, bspw. die CPU-Floating-Point Performance und keine realistischen Applikationen untersuchen\cite{scheuner_function-as--service_2020}. Vergleiche mit der Performance von Container-Anwendungen sind nur wenige zu finden.  

McGrath und Brenner untersuchten die Performance von Serverless-\linebreak Funktionen durch mehrere verschiedene Tests\cite{mcgrath_serverless_2017}. Unter anderem wurde ein Nebenläufigkeits-Test (concurrency test) vorgestellt, bei dem eine Funktion, die sofort terminiert, von einer linear ansteigenden Anzahl an Clients so oft wie möglich hintereinander aufgerufen wurde. Gemessen wurde dabei die Anzahl der Antworten pro Sekunde (responses per second). Ziel dieses Tests war, die performante Ausführung einer skalierten Funktion zu messen. Das Ergebnis zeigte unterschiedliches Skalierungsverhalten für diverse Cloud-Anbieter; bei \ac{AWS} Lambda wurde ein weitestgehend linearer Anstieg festgestellt. 
Für zukünftige Forschung werden von den Autoren noch andere die Performance von Serverless-Funktionen beeinflussende Aspekte genannt, wie die Ausführung mehrerer Funktionen anstatt nur einer einzigen (single function execution), die Code-Größe einer Serverless-Funktion und unterschiedliche Funktionsgrößen (in der Studie wurde lediglich 512MB RAM verwendet).

Hendrickson et al. verglichen in ihrer Arbeit die Performance von \ac{AWS} Lambda mit der eines in \ac{AWS} Beanstalk laufenden Containers\cite{hendrickson_serverless_2017}. Dazu sendeten die Forscher jeweils 100 RPC-Requests an beide Services, wobei jeder Service eine Laufzeit von 200ms beanspruchte. Die Durchführung des Tests ergab, dass die Antwortzeit bei Lambda mit einem Medianwert von 1,6 Sekunden deutlich vor der von Beanstalk mit bis zu 20 Sekunden liegt. Als Grund dafür wird angeführt, dass Lambda innerhalb von einigen Millisekunden 100 Instanzen starte, auf die die Last verteilt wurde, während bei Beanstalk alle Anfragen von einer einzigen Instanz bearbeitet werden mussten. Denn die Beanstalk-Anwendung wurde nicht hoch skaliert, obwohl alle Einstellungen getroffen wurden, damit die Skalierung so schnell wie möglich erfolgen kann. Während es laut den Forschern bei Beanstalk zwanzig verschiedene Einstellungsmöglichkeiten zum Skalierungsverhalten gäbe, übernähme Lambda dies vollkommen automatisch.
Allerdings wird auch deutlich, dass die Latenz von Lambda-Funktionen bei einer normalen Last deutlich über der eines Containers liegt. Mit der gleichen Testumgebung und unter leichter Last, performe Lambda zehn mal schlechter als Beanstalk. Da dieser Test im Jahre 2017 durchgeführt wurde, stimmen diese Ergebnisse aber vermutlich nicht mit den in der Zwischenzeit an Lambda vorgenommenen Performance-Verbesserungen überein.

Villamizar et al. verglichen die Performance und infrastrukturellen Kosten einer auf verschiedene Arten deployten Applikation\cite{villamizar_infrastructure_2016}. Die Anwendung wurde als Monolith und als Microservices-Architektur sowohl mit, als auch ohne \ac{AWS} Lambda betrieben. Dabei zeigte sich, dass die Kosten für den Betrieb bei einer auf Lambda basierenden Microservices-Architektur mehr als 70 Prozent geringer ausfielen, als bei einem Monolithen oder wenn die Microservices von den Entwicklern manuell gemanaged wurden. Ebenfalls wurde die Performance der verschiedenen Ansätze mittels der Metrik der durchschnittlichen Antwortzeit (average response time - ART) verglichen. Es wurde deutlich, dass Lambda in unterschiedlichen Test-Szenarien nahezu die gleich ART beibehält, was auf dessen Skalierungsmöglichkeiten zurückgeführt wurde. Des Weiteren performte Lambda teilweise besser als die monolithische Architektur und deutlich besser als die selbst-gemanagten Microservices.

Verschiedene Server-Hosting Technologien, darunter auch \ac{AWS} Lambda und \ac{AWS} Fargate, wurden 2020 von Jain et al. auf ihre Performance hin untersucht\cite{jain_performance_2020}. Ähnlich zu dieser Arbeit wurde eine Notiz-Anwendung deployed, allerdings handelte es sich dabei um eine Frontend-Anwendung. Als Test-Metriken wurde sich demnach ausschließlich auf solche beschränkt, die das Rendering der Applikation im Web-Browser des Endbenutzer betreffen, bspw. die Page-Load-Time und Start-Render-Time. Dies steht im Gegensatz zu dieser Arbeit, bei dem ausschließlich Backend-Technologien miteinander verglichen werden. Die Ergebnisse der Arbeit zeigten dennoch, dass \ac{ECS} mit Fargate deutlich besser performte als \ac{AWS} Lambda und auch \ac{ECS} mit \ac{EC2} übertraf. Es wurden jedoch weder Stress-Tests der Anwendungen durchgeführt, noch die Kostenunterschiede untersucht.

Alex DeBrie untersuchte 2019 in einem Artikel auf seiner Blog-Seite die Performance von \ac{ECS} Fargate-Containern und \ac{AWS} Lambda\cite{debrie_aws_2019}. Dazu wurde ähnlich wie in dieser Arbeit ein HTTP-Endpunkt deployed und 15.000 Requests an jede Variante gesendet. Die HTTP-Endpunkte sendeten dann den Payload des Requests an den Amazon Simple Notification Service (SNS) bevor die Antwort zurückgesendet wurde. Zur Auswertung wurden Percentile der Antwortzeit verwendet. Es zeigte sich, dass die Fargate-Container den Lambda-Funktionen deutlich in der Performance überlegen waren. Allerdings wurde nur eine einzige Konfiguration für beide Technologien getestet.

Nach der Betrachtung der relevanten Arbeiten und Artikel zeigt sich also, dass es bisher noch keine Studie gibt, die die Performance von Containern und Serverless-Funktionen anhand mehrerer Konfigurationen und mit verschiedenen Performance-Tests im Kontext eines HTTP-REST-Backends evaluiert. Diese Lücke versucht die vorliegende Arbeit zu schließen, indem reproduzierbare Test mit verschiedenen Konfigurationen durchgeführt werden und auch auf die Kosten der beiden Technologien eingegangen wird.